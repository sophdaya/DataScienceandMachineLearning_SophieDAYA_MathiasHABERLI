{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98U8TyduZyYr"
      },
      "source": [
        "# IMPORT THE DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rJqVnvEWCvT",
        "outputId": "e3f46929-5292-4a11-d343-3c62a5d22a84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pkhzSuoVLMs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Adjust the path based on where you've stored the files\n",
        "training_data = '/content/drive/MyDrive/Colab Notebooks/2024_Data_science/Final_Project_Kaggle_Competition/training_data.csv'\n",
        "unlabelled_data = '/content/drive/MyDrive/Colab Notebooks/2024_Data_science/Final_Project_Kaggle_Competition/unlabelled_test_data.csv'\n",
        "sample_submission = '/content/drive/MyDrive/Colab Notebooks/2024_Data_science/Final_Project_Kaggle_Competition/sample_submission.csv'\n",
        "\n",
        "training_data_pd = pd.read_csv(training_data)\n",
        "unlabelled_data_pd = pd.read_csv(unlabelled_data)\n",
        "sample_submission_pd = pd.read_csv(sample_submission)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WfFpK0NGWyJc",
        "outputId": "ab007831-946e-49e5-ae2b-fa5034efaf48"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "        id                                           sentence difficulty\n",
              "0        0  les coûts kilométriques réels peuvent diverger...         C1\n",
              "1        1  le bleu cest ma couleur préférée mais je naime...         A1\n",
              "2        2  le test de niveau en français est sur le site ...         A1\n",
              "3        3             estce que ton mari est aussi de boston         A1\n",
              "4        4  dans les écoles de commerce dans les couloirs ...         B1\n",
              "...    ...                                                ...        ...\n",
              "4795  4795  cest pourquoi il décida de remplacer les habit...         B2\n",
              "4796  4796  il avait une de ces pâleurs splendides qui don...         C1\n",
              "4797  4797  et le premier samedi de chaque mois venez renc...         A2\n",
              "4798  4798  les coûts liés à la journalisation nétant pas ...         C2\n",
              "4799  4799  sur le sable la mer haletait de toute la respi...         C2\n",
              "\n",
              "[4800 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7c7da5be-54c4-4028-8dab-912f9011da97\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentence</th>\n",
              "      <th>difficulty</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>les coûts kilométriques réels peuvent diverger...</td>\n",
              "      <td>C1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>le bleu cest ma couleur préférée mais je naime...</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>le test de niveau en français est sur le site ...</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>estce que ton mari est aussi de boston</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>dans les écoles de commerce dans les couloirs ...</td>\n",
              "      <td>B1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4795</th>\n",
              "      <td>4795</td>\n",
              "      <td>cest pourquoi il décida de remplacer les habit...</td>\n",
              "      <td>B2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4796</th>\n",
              "      <td>4796</td>\n",
              "      <td>il avait une de ces pâleurs splendides qui don...</td>\n",
              "      <td>C1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4797</th>\n",
              "      <td>4797</td>\n",
              "      <td>et le premier samedi de chaque mois venez renc...</td>\n",
              "      <td>A2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4798</th>\n",
              "      <td>4798</td>\n",
              "      <td>les coûts liés à la journalisation nétant pas ...</td>\n",
              "      <td>C2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4799</th>\n",
              "      <td>4799</td>\n",
              "      <td>sur le sable la mer haletait de toute la respi...</td>\n",
              "      <td>C2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4800 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7c7da5be-54c4-4028-8dab-912f9011da97')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7c7da5be-54c4-4028-8dab-912f9011da97 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7c7da5be-54c4-4028-8dab-912f9011da97');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9d4c4f4a-25cd-4db3-bfb7-d8fe72998a1b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9d4c4f4a-25cd-4db3-bfb7-d8fe72998a1b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9d4c4f4a-25cd-4db3-bfb7-d8fe72998a1b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "training_data_pd",
              "summary": "{\n  \"name\": \"training_data_pd\",\n  \"rows\": 4800,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1385,\n        \"min\": 0,\n        \"max\": 4799,\n        \"num_unique_values\": 4800,\n        \"samples\": [\n          596,\n          3370,\n          3048\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4789,\n        \"samples\": [\n          \"nous sommes en retard\",\n          \"ensuite suivre lexemple des viticulteurs qui ont su eux se remettre en cause op\\u00e9rant une r\\u00e9forme qui porte aujourdhui ses fruits\",\n          \"boum voil\\u00e0 on arrive on veut faire du v\\u00e9lo en famille et on a les vacances\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"difficulty\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"C1\",\n          \"A1\",\n          \"C2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "        id                                           sentence\n",
              "0        0  nous dûmes nous excuser des propos que nous eû...\n",
              "1        1  vous ne pouvez pas savoir le plaisir que jai d...\n",
              "2        2  et paradoxalement boire froid nest pas la bonn...\n",
              "3        3  ce nest pas étonnant car cest une saison mysté...\n",
              "4        4  le corps de golo luimême dune essence aussi su...\n",
              "...    ...                                                ...\n",
              "1195  1195  cest un phénomène qui trouve une accélération ...\n",
              "1196  1196  je vais parler au serveur et voir si on peut d...\n",
              "1197  1197  il nétait pas comme tant de gens qui par pares...\n",
              "1198  1198       ils deviennent dangereux pour notre économie\n",
              "1199  1199  son succès a généré beaucoup de réactions néga...\n",
              "\n",
              "[1200 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-34f83f63-845f-4f55-a6b1-023ff7829a5f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>nous dûmes nous excuser des propos que nous eû...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>vous ne pouvez pas savoir le plaisir que jai d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>et paradoxalement boire froid nest pas la bonn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>ce nest pas étonnant car cest une saison mysté...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>le corps de golo luimême dune essence aussi su...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1195</th>\n",
              "      <td>1195</td>\n",
              "      <td>cest un phénomène qui trouve une accélération ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1196</th>\n",
              "      <td>1196</td>\n",
              "      <td>je vais parler au serveur et voir si on peut d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1197</th>\n",
              "      <td>1197</td>\n",
              "      <td>il nétait pas comme tant de gens qui par pares...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1198</th>\n",
              "      <td>1198</td>\n",
              "      <td>ils deviennent dangereux pour notre économie</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1199</th>\n",
              "      <td>1199</td>\n",
              "      <td>son succès a généré beaucoup de réactions néga...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1200 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-34f83f63-845f-4f55-a6b1-023ff7829a5f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-34f83f63-845f-4f55-a6b1-023ff7829a5f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-34f83f63-845f-4f55-a6b1-023ff7829a5f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9b86f172-6628-4841-81b3-2051d2003741\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9b86f172-6628-4841-81b3-2051d2003741')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9b86f172-6628-4841-81b3-2051d2003741 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "unlabelled_data_pd",
              "summary": "{\n  \"name\": \"unlabelled_data_pd\",\n  \"rows\": 1200,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 346,\n        \"min\": 0,\n        \"max\": 1199,\n        \"num_unique_values\": 1200,\n        \"samples\": [\n          1178,\n          865,\n          101\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1200,\n        \"samples\": [\n          \"cest mon restaurant pr\\u00e9f\\u00e9r\\u00e9 jy suis all\\u00e9 deux fois cette semaine\",\n          \"avec leur directrice patricia rodriguez les \\u00e9l\\u00e8ves de la grande section de maternelle ne se lassent pas de ce spectacle\",\n          \"pour y rem\\u00e9dier \\u00e0 leur \\u00e9chelle des militants ont commenc\\u00e9 \\u00e0 organiser leurs propres fili\\u00e8res dimportation d\\u00e8s la fin des ann\\u00e9es 1960\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "        id difficulty\n",
              "0        0         A1\n",
              "1        1         A1\n",
              "2        2         A1\n",
              "3        3         A1\n",
              "4        4         A1\n",
              "...    ...        ...\n",
              "1195  1195         A1\n",
              "1196  1196         A1\n",
              "1197  1197         A1\n",
              "1198  1198         A1\n",
              "1199  1199         A1\n",
              "\n",
              "[1200 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8081947f-2f0a-4732-8a24-09b711c28dd5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>difficulty</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1195</th>\n",
              "      <td>1195</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1196</th>\n",
              "      <td>1196</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1197</th>\n",
              "      <td>1197</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1198</th>\n",
              "      <td>1198</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1199</th>\n",
              "      <td>1199</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1200 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8081947f-2f0a-4732-8a24-09b711c28dd5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8081947f-2f0a-4732-8a24-09b711c28dd5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8081947f-2f0a-4732-8a24-09b711c28dd5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-dd8170f6-502d-4f33-ac63-4431ddfaa429\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dd8170f6-502d-4f33-ac63-4431ddfaa429')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-dd8170f6-502d-4f33-ac63-4431ddfaa429 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "sample_submission_pd",
              "summary": "{\n  \"name\": \"sample_submission_pd\",\n  \"rows\": 1200,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 346,\n        \"min\": 0,\n        \"max\": 1199,\n        \"num_unique_values\": 1200,\n        \"samples\": [\n          1178,\n          865,\n          101\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"difficulty\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"A1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "display(training_data_pd)\n",
        "display(unlabelled_data_pd)\n",
        "display(sample_submission_pd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki3WtVRavMxO"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlP7Zl0FvQ5k",
        "outputId": "1b56bf62-3a5a-4a65-9de6-1b4650fae5f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id                                           sentence difficulty\n",
            "0   0  les coûts kilométriques réels peuvent diverger...         C1\n",
            "1   1  le bleu cest ma couleur préférée mais je naime...         A1\n",
            "2   2  le test de niveau en français est sur le site ...         A1\n",
            "3   3             estce que ton mari est aussi de boston         A1\n",
            "4   4  dans les écoles de commerce dans les couloirs ...         B1\n",
            "   id                                           sentence\n",
            "0   0  nous dûmes nous excuser des propos que nous eû...\n",
            "1   1  vous ne pouvez pas savoir le plaisir que jai d...\n",
            "2   2  et paradoxalement boire froid nest pas la bonn...\n",
            "3   3  ce nest pas étonnant car cest une saison mysté...\n",
            "4   4  le corps de golo luimême dune essence aussi su...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "\n",
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Remove extra whitespace\n",
        "    text = \" \".join(text.split())\n",
        "    return text\n",
        "\n",
        "# Apply cleaning function to the sentence column in both datasets\n",
        "training_data_pd['sentence'] = training_data_pd['sentence'].apply(clean_text)\n",
        "unlabelled_data_pd['sentence'] = unlabelled_data_pd['sentence'].apply(clean_text)\n",
        "\n",
        "# Display the first few rows of the cleaned data to confirm changes\n",
        "print(training_data_pd.head())\n",
        "print(unlabelled_data_pd.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3jBDEwmzxAX",
        "outputId": "9df574de-4877-4900-97a2-ae0bea9b95c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate==0.29.3 in /usr/local/lib/python3.10/dist-packages (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.29.3) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.29.3) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.29.3) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.29.3) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.29.3) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.29.3) (0.23.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.29.3) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.29.3) (12.4.127)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.29.3) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.29.3) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.29.3) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.29.3) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.29.3) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.29.3) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.29.3) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.29.3) (1.3.0)\n",
            "Requirement already satisfied: transformers[torch]==4.40.1 in /usr/local/lib/python3.10/dist-packages (4.40.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.40.1) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.40.1) (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.40.1) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.40.1) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.40.1) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.40.1) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.40.1) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.40.1) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.40.1) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.40.1) (4.66.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.40.1) (2.2.1+cu121)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.40.1) (0.29.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]==4.40.1) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]==4.40.1) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]==4.40.1) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]==4.40.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]==4.40.1) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]==4.40.1) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]==4.40.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]==4.40.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]==4.40.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]==4.40.1) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]==4.40.1) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]==4.40.1) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]==4.40.1) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]==4.40.1) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]==4.40.1) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]==4.40.1) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]==4.40.1) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]==4.40.1) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]==4.40.1) (12.4.127)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]==4.40.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]==4.40.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]==4.40.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]==4.40.1) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]==4.40.1) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]==4.40.1) (1.3.0)\n",
            "0.29.3\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "\"\"\"\n",
        "!pip install --no-cache-dir accelerate==0.29.3\n",
        "!pip install --no-cache-dir transformers[torch]==4.40.1\n",
        "\n",
        "import accelerate\n",
        "print(accelerate.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltSTcEeyY7jx",
        "outputId": "90c63ce5-e626-4baa-f08f-75a8b88b91ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accelerate version: 0.29.3\n",
            "Transformers version: 4.40.1\n"
          ]
        }
      ],
      "source": [
        "import accelerate\n",
        "import transformers\n",
        "print(\"Accelerate version:\", accelerate.__version__)\n",
        "print(\"Transformers version:\", transformers.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YUPq0yh0jg7",
        "outputId": "5ba5a186-457e-4b70-e271-045cf92e7269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=6)  # Assuming 6 labels for CEFR levels A1, A2, B1, B2, C1, C2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3UvhtRxvjcx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "47c34849-4fe4-42a6-c47e-c38970c346f3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\\nfrom sklearn.model_selection import train_test_split\\nfrom datasets import Dataset, load_metric\\nimport numpy as np\\n\\n# Load the tokenizer\\ntokenizer = DistilBertTokenizer.from_pretrained(\\'distilbert-base-uncased\\')\\n\\n# Function to encode data\\ndef encode_data(tokenizer, df):\\n    texts = df[\\'sentence\\'].tolist()\\n    labels = df[\\'difficulty\\'].map({\\'A1\\': 0, \\'A2\\': 1, \\'B1\\': 2, \\'B2\\': 3, \\'C1\\': 4, \\'C2\\': 5}).tolist()\\n    encodings = tokenizer(texts, truncation=True, padding=\\'max_length\\', max_length=128)\\n    return Dataset.from_dict({\\n        \\'input_ids\\': encodings[\\'input_ids\\'],\\n        \\'attention_mask\\': encodings[\\'attention_mask\\'],\\n        \\'labels\\': labels\\n    })\\n\\n# Split the DataFrame into training and validation sets\\ntrain_data, val_data = train_test_split(training_data_pd, test_size=0.10, random_state=42)\\n\\n# Tokenize and prepare datasets\\ntrain_dataset = encode_data(tokenizer, train_data)\\nval_dataset = encode_data(tokenizer, val_data)\\n\\n# Load the model\\nmodel = DistilBertForSequenceClassification.from_pretrained(\\'distilbert-base-uncased\\', num_labels=6)\\n\\n# Metric for evaluation\\ndef compute_metrics(eval_pred):\\n    metric = load_metric(\"accuracy\")\\n    logits, labels = eval_pred\\n    predictions = np.argmax(logits, axis=-1)\\n    accuracy = metric.compute(predictions=predictions, references=labels)\\n    return {\"accuracy\": accuracy[\\'accuracy\\']}\\n\\n# Define training arguments\\ntraining_args = TrainingArguments(\\n    output_dir=\\'./results\\',\\n    num_train_epochs=6,  # Slight adjustment if needed based on results\\n    per_device_train_batch_size=12,  # A bit smaller to adjust gradient updates\\n    warmup_steps=500,  # Adjust based on the total number of steps (num_epochs * total_data / batch_size)\\n    weight_decay=0.02,  # Good for regularization\\n    logging_dir=\\'./logs\\',\\n    logging_steps=50,  # Less frequent to reduce logging overhead\\n    learning_rate=3e-5,  # Adjusted for potentially better convergence\\n    fp16=False,  # Make sure your hardware supports FP16 for this to be effective\\n    evaluation_strategy=\"epoch\",  # Change to steps if you need more frequent evaluation\\n    save_strategy=\"epoch\",\\n    load_best_model_at_end=True,\\n    metric_for_best_model=\\'accuracy\\',\\n    greater_is_better=True\\n)\\n\\n# Initialize the Trainer with added compute_metrics for dynamic metric calculation\\ntrainer = Trainer(\\n    model=model,\\n    args=training_args,\\n    train_dataset=train_dataset,\\n    eval_dataset=val_dataset,\\n    compute_metrics=compute_metrics  # Ensure metrics are computed during evaluation\\n)\\n\\n# Train the model\\ntrainer.train()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# 52.5%\n",
        "\n",
        "\"\"\"\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset, load_metric\n",
        "import numpy as np\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Function to encode data\n",
        "def encode_data(tokenizer, df):\n",
        "    texts = df['sentence'].tolist()\n",
        "    labels = df['difficulty'].map({'A1': 0, 'A2': 1, 'B1': 2, 'B2': 3, 'C1': 4, 'C2': 5}).tolist()\n",
        "    encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=128)\n",
        "    return Dataset.from_dict({\n",
        "        'input_ids': encodings['input_ids'],\n",
        "        'attention_mask': encodings['attention_mask'],\n",
        "        'labels': labels\n",
        "    })\n",
        "\n",
        "# Split the DataFrame into training and validation sets\n",
        "train_data, val_data = train_test_split(training_data_pd, test_size=0.10, random_state=42)\n",
        "\n",
        "# Tokenize and prepare datasets\n",
        "train_dataset = encode_data(tokenizer, train_data)\n",
        "val_dataset = encode_data(tokenizer, val_data)\n",
        "\n",
        "# Load the model\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=6)\n",
        "\n",
        "# Metric for evaluation\n",
        "def compute_metrics(eval_pred):\n",
        "    metric = load_metric(\"accuracy\")\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    accuracy = metric.compute(predictions=predictions, references=labels)\n",
        "    return {\"accuracy\": accuracy['accuracy']}\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=6,  # Slight adjustment if needed based on results\n",
        "    per_device_train_batch_size=12,  # A bit smaller to adjust gradient updates\n",
        "    warmup_steps=500,  # Adjust based on the total number of steps (num_epochs * total_data / batch_size)\n",
        "    weight_decay=0.02,  # Good for regularization\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,  # Less frequent to reduce logging overhead\n",
        "    learning_rate=3e-5,  # Adjusted for potentially better convergence\n",
        "    fp16=False,  # Make sure your hardware supports FP16 for this to be effective\n",
        "    evaluation_strategy=\"epoch\",  # Change to steps if you need more frequent evaluation\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='accuracy',\n",
        "    greater_is_better=True\n",
        ")\n",
        "\n",
        "# Initialize the Trainer with added compute_metrics for dynamic metric calculation\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics  # Ensure metrics are computed during evaluation\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGHPIpIq8OiZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a0093ba-cd09-48db-ca02-cb89b3407f48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.13.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.29)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.3)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n"
          ]
        }
      ],
      "source": [
        "pip install optuna\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1v9XGvgC0iG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0587a54-67d7-4fe9-f4ec-c53d478846e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCDA-8F6_XMA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dc206c5-756b-49f9-d466-9aa58d72abeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Light CamemBERT first try, 56%\n",
        "\"\"\"\n",
        "from transformers import CamembertTokenizer, CamembertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset, load_metric\n",
        "import numpy as np\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
        "\n",
        "# Function to encode data\n",
        "def encode_data(tokenizer, df):\n",
        "    texts = df['sentence'].tolist()\n",
        "    labels = df['difficulty'].map({'A1': 0, 'A2': 1, 'B1': 2, 'B2': 3, 'C1': 4, 'C2': 5}).tolist()\n",
        "    encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=128)\n",
        "    return Dataset.from_dict({\n",
        "        'input_ids': encodings['input_ids'],\n",
        "        'attention_mask': encodings['attention_mask'],\n",
        "        'labels': labels\n",
        "    })\n",
        "\n",
        "# Split the DataFrame into training and validation sets\n",
        "train_data, val_data = train_test_split(training_data_pd, test_size=0.10, random_state=42)\n",
        "\n",
        "# Tokenize and prepare datasets\n",
        "train_dataset = encode_data(tokenizer, train_data)\n",
        "val_dataset = encode_data(tokenizer, val_data)\n",
        "\n",
        "# Load the model\n",
        "model = CamembertForSequenceClassification.from_pretrained('camembert-base', num_labels=6)\n",
        "\n",
        "# Metric for evaluation\n",
        "def compute_metrics(eval_pred):\n",
        "    metric = load_metric(\"accuracy\")\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    accuracy = metric.compute(predictions=predictions, references=labels)\n",
        "    return {\"accuracy\": accuracy['accuracy']}\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=5,  # Slight adjustment if needed based on results\n",
        "    per_device_train_batch_size=12,  # A bit smaller to adjust gradient updates\n",
        "    warmup_steps=500,  # Adjust based on the total number of steps (num_epochs * total_data / batch_size)\n",
        "    weight_decay=0.02,  # Good for regularization\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,  # Less frequent to reduce logging overhead\n",
        "    learning_rate=3e-5,  # Adjusted for potentially better convergence\n",
        "    fp16=True,  # Make sure your hardware supports FP16 for this to be effective\n",
        "    evaluation_strategy=\"epoch\",  # Change to steps if you need more frequent evaluation\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='accuracy',\n",
        "    greater_is_better=True\n",
        ")\n",
        "\n",
        "# Initialize the Trainer with added compute_metrics for dynamic metric calculation\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics  # Ensure metrics are computed during evaluation\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "from transformers import CamembertTokenizer, CamembertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset, load_metric\n",
        "import numpy as np\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
        "\n",
        "# Function to encode data\n",
        "def encode_data(tokenizer, df):\n",
        "    texts = df['sentence'].tolist()\n",
        "    labels = df['difficulty'].map({'A1': 0, 'A2': 1, 'B1': 2, 'B2': 3, 'C1': 4, 'C2': 5}).tolist()\n",
        "    encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=512)\n",
        "    return Dataset.from_dict({\n",
        "        'input_ids': encodings['input_ids'],\n",
        "        'attention_mask': encodings['attention_mask'],\n",
        "        'labels': labels\n",
        "    })\n",
        "\n",
        "# Split the DataFrame into training and validation sets\n",
        "train_data, val_data = train_test_split(training_data_pd, test_size=0.10, random_state=42)\n",
        "\n",
        "# Tokenize and prepare datasets\n",
        "train_dataset = encode_data(tokenizer, train_data)\n",
        "val_dataset = encode_data(tokenizer, val_data)\n",
        "\n",
        "# Load the model\n",
        "model = CamembertForSequenceClassification.from_pretrained('camembert-base', num_labels=6)\n",
        "\n",
        "# Metric for evaluation\n",
        "def compute_metrics(eval_pred):\n",
        "    metric = load_metric(\"accuracy\")\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    accuracy = metric.compute(predictions=predictions, references=labels)\n",
        "    return {\"accuracy\": accuracy['accuracy']}\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=4,  # Slight adjustment if needed based on results\n",
        "    per_device_train_batch_size=12,  # A bit smaller to adjust gradient updates\n",
        "    warmup_steps=500,  # Adjust based on the total number of steps (num_epochs * total_data / batch_size)\n",
        "    weight_decay=0.02,  # Good for regularization\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,  # Less frequent to reduce logging overhead\n",
        "    learning_rate=3e-5,  # Adjusted for potentially better convergence\n",
        "    fp16=True,  # Make sure your hardware supports FP16 for this to be effective\n",
        "    evaluation_strategy=\"epoch\",  # Change to steps if you need more frequent evaluation\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='accuracy',\n",
        "    greater_is_better=True\n",
        ")\n",
        "\n",
        "# Initialize the Trainer with added compute_metrics for dynamic metric calculation\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics  # Ensure metrics are computed during evaluation\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "LV1-Q-ltOGxm",
        "outputId": "f5e6f020-9f50-4770-c9e7-455e0e8671bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom transformers import CamembertTokenizer, CamembertForSequenceClassification, Trainer, TrainingArguments\\nfrom sklearn.model_selection import train_test_split\\nfrom datasets import Dataset, load_metric\\nimport numpy as np\\n\\n# Load the tokenizer\\ntokenizer = CamembertTokenizer.from_pretrained(\\'camembert-base\\')\\n\\n# Function to encode data\\ndef encode_data(tokenizer, df):\\n    texts = df[\\'sentence\\'].tolist()\\n    labels = df[\\'difficulty\\'].map({\\'A1\\': 0, \\'A2\\': 1, \\'B1\\': 2, \\'B2\\': 3, \\'C1\\': 4, \\'C2\\': 5}).tolist()\\n    encodings = tokenizer(texts, truncation=True, padding=\\'max_length\\', max_length=512)\\n    return Dataset.from_dict({\\n        \\'input_ids\\': encodings[\\'input_ids\\'],\\n        \\'attention_mask\\': encodings[\\'attention_mask\\'],\\n        \\'labels\\': labels\\n    })\\n\\n# Split the DataFrame into training and validation sets\\ntrain_data, val_data = train_test_split(training_data_pd, test_size=0.10, random_state=42)\\n\\n# Tokenize and prepare datasets\\ntrain_dataset = encode_data(tokenizer, train_data)\\nval_dataset = encode_data(tokenizer, val_data)\\n\\n# Load the model\\nmodel = CamembertForSequenceClassification.from_pretrained(\\'camembert-base\\', num_labels=6)\\n\\n# Metric for evaluation\\ndef compute_metrics(eval_pred):\\n    metric = load_metric(\"accuracy\")\\n    logits, labels = eval_pred\\n    predictions = np.argmax(logits, axis=-1)\\n    accuracy = metric.compute(predictions=predictions, references=labels)\\n    return {\"accuracy\": accuracy[\\'accuracy\\']}\\n\\n# Define training arguments\\ntraining_args = TrainingArguments(\\n    output_dir=\\'./results\\',\\n    num_train_epochs=4,  # Slight adjustment if needed based on results\\n    per_device_train_batch_size=12,  # A bit smaller to adjust gradient updates\\n    warmup_steps=500,  # Adjust based on the total number of steps (num_epochs * total_data / batch_size)\\n    weight_decay=0.02,  # Good for regularization\\n    logging_dir=\\'./logs\\',\\n    logging_steps=50,  # Less frequent to reduce logging overhead\\n    learning_rate=3e-5,  # Adjusted for potentially better convergence\\n    fp16=True,  # Make sure your hardware supports FP16 for this to be effective\\n    evaluation_strategy=\"epoch\",  # Change to steps if you need more frequent evaluation\\n    save_strategy=\"epoch\",\\n    load_best_model_at_end=True,\\n    metric_for_best_model=\\'accuracy\\',\\n    greater_is_better=True\\n)\\n\\n# Initialize the Trainer with added compute_metrics for dynamic metric calculation\\ntrainer = Trainer(\\n    model=model,\\n    args=training_args,\\n    train_dataset=train_dataset,\\n    eval_dataset=val_dataset,\\n    compute_metrics=compute_metrics  # Ensure metrics are computed during evaluation\\n)\\n\\n# Train the model\\ntrainer.train()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import CamembertTokenizer, CamembertForSequenceClassification, Trainer, TrainingArguments\n",
        "from transformers import EarlyStoppingCallback\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset, load_metric\n",
        "import numpy as np\n",
        "\"\"\"\n",
        "# Load your datasets\n",
        "training_data_pd = pd.read_csv(training_data)\n",
        "unlabelled_data_pd = pd.read_csv(unlabelled_data)\n",
        "sample_submission_pd = pd.read_csv(sample_submission)\n",
        "\n",
        "# Load the tokenizer and the model\n",
        "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
        "model = CamembertForSequenceClassification.from_pretrained('camembert-base', num_labels=6)\n",
        "\n",
        "# Encode data function\n",
        "def encode_data(tokenizer, df):\n",
        "    texts = df['sentence'].tolist()\n",
        "    labels = df['difficulty'].map({'A1': 0, 'A2': 1, 'B1': 2, 'B2': 3, 'C1': 4, 'C2': 5}).tolist()\n",
        "    encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=128)\n",
        "\n",
        "    df_encoded = pd.DataFrame({\n",
        "        'input_ids': list(encodings['input_ids']),\n",
        "        'attention_mask': list(encodings['attention_mask']),\n",
        "        'labels': labels\n",
        "    })\n",
        "\n",
        "    return Dataset.from_pandas(df_encoded)\n",
        "\n",
        "# Compute metrics function\n",
        "def compute_metrics(eval_pred):\n",
        "    metric = load_metric(\"accuracy\")\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "    return {'accuracy': metric.compute(predictions=predictions, references=labels)['accuracy']}\n",
        "\n",
        "# Grid search function\n",
        "def grid_search(tokenizer, model, dataset, param_grid):\n",
        "    results = []\n",
        "    for batch_size in param_grid['batch_sizes']:\n",
        "        for learning_rate in param_grid['learning_rates']:\n",
        "            for test_size in param_grid['test_sizes']:\n",
        "                train_data, val_data = train_test_split(dataset, test_size=test_size, random_state=42)\n",
        "                train_dataset = encode_data(tokenizer, train_data)\n",
        "                val_dataset = encode_data(tokenizer, val_data)\n",
        "\n",
        "                training_args = TrainingArguments(\n",
        "                    output_dir='./results',\n",
        "                    num_train_epochs=7,\n",
        "                    per_device_train_batch_size=batch_size,\n",
        "                    learning_rate=learning_rate,\n",
        "                    warmup_steps=500,\n",
        "                    weight_decay=0.02,\n",
        "                    logging_dir='./logs',\n",
        "                    evaluation_strategy='epoch',\n",
        "                    save_strategy='epoch',\n",
        "                    load_best_model_at_end=True,\n",
        "                    metric_for_best_model='accuracy',\n",
        "                    greater_is_better=True,\n",
        "                    fp16=True\n",
        "                )\n",
        "\n",
        "                trainer = Trainer(\n",
        "                    model=model,\n",
        "                    args=training_args,\n",
        "                    train_dataset=train_dataset,\n",
        "                    eval_dataset=val_dataset,\n",
        "                    compute_metrics=compute_metrics,\n",
        "                    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        "                )\n",
        "\n",
        "                trainer.train()\n",
        "\n",
        "                eval_results = trainer.evaluate()\n",
        "                results.append({\n",
        "                    'batch_size': batch_size,\n",
        "                    'learning_rate': learning_rate,\n",
        "                    'test_size': test_size,\n",
        "                    'accuracy': eval_results['eval_accuracy']\n",
        "                })\n",
        "\n",
        "    best_result = max(results, key=lambda x: x['accuracy'])\n",
        "    return best_result, results\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'batch_sizes': [8, 16, 32],\n",
        "    'learning_rates': [2e-5, 3e-5, 5e-5],\n",
        "    'test_sizes': [0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "# Execute the grid search\n",
        "best_parameters, all_results = grid_search(tokenizer, model, training_data_pd, param_grid)\n",
        "print(\"Best Parameters:\", best_parameters)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "xQIUN0O6Zq2h",
        "outputId": "3f9a9c73-34a2-488a-bbae-11d9506ba335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Load your datasets\\ntraining_data_pd = pd.read_csv(training_data)\\nunlabelled_data_pd = pd.read_csv(unlabelled_data)\\nsample_submission_pd = pd.read_csv(sample_submission)\\n\\n# Load the tokenizer and the model\\ntokenizer = CamembertTokenizer.from_pretrained(\\'camembert-base\\')\\nmodel = CamembertForSequenceClassification.from_pretrained(\\'camembert-base\\', num_labels=6)\\n\\n# Encode data function\\ndef encode_data(tokenizer, df):\\n    texts = df[\\'sentence\\'].tolist()\\n    labels = df[\\'difficulty\\'].map({\\'A1\\': 0, \\'A2\\': 1, \\'B1\\': 2, \\'B2\\': 3, \\'C1\\': 4, \\'C2\\': 5}).tolist()\\n    encodings = tokenizer(texts, truncation=True, padding=\\'max_length\\', max_length=128)\\n\\n    df_encoded = pd.DataFrame({\\n        \\'input_ids\\': list(encodings[\\'input_ids\\']),\\n        \\'attention_mask\\': list(encodings[\\'attention_mask\\']),\\n        \\'labels\\': labels\\n    })\\n\\n    return Dataset.from_pandas(df_encoded)\\n\\n# Compute metrics function\\ndef compute_metrics(eval_pred):\\n    metric = load_metric(\"accuracy\")\\n    logits, labels = eval_pred\\n    predictions = np.argmax(logits, axis=1)\\n    return {\\'accuracy\\': metric.compute(predictions=predictions, references=labels)[\\'accuracy\\']}\\n\\n# Grid search function\\ndef grid_search(tokenizer, model, dataset, param_grid):\\n    results = []\\n    for batch_size in param_grid[\\'batch_sizes\\']:\\n        for learning_rate in param_grid[\\'learning_rates\\']:\\n            for test_size in param_grid[\\'test_sizes\\']:\\n                train_data, val_data = train_test_split(dataset, test_size=test_size, random_state=42)\\n                train_dataset = encode_data(tokenizer, train_data)\\n                val_dataset = encode_data(tokenizer, val_data)\\n\\n                training_args = TrainingArguments(\\n                    output_dir=\\'./results\\',\\n                    num_train_epochs=7,\\n                    per_device_train_batch_size=batch_size,\\n                    learning_rate=learning_rate,\\n                    warmup_steps=500,\\n                    weight_decay=0.02,\\n                    logging_dir=\\'./logs\\',\\n                    evaluation_strategy=\\'epoch\\',\\n                    save_strategy=\\'epoch\\',\\n                    load_best_model_at_end=True,\\n                    metric_for_best_model=\\'accuracy\\',\\n                    greater_is_better=True,\\n                    fp16=True\\n                )\\n\\n                trainer = Trainer(\\n                    model=model,\\n                    args=training_args,\\n                    train_dataset=train_dataset,\\n                    eval_dataset=val_dataset,\\n                    compute_metrics=compute_metrics,\\n                    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\\n                )\\n\\n                trainer.train()\\n\\n                eval_results = trainer.evaluate()\\n                results.append({\\n                    \\'batch_size\\': batch_size,\\n                    \\'learning_rate\\': learning_rate,\\n                    \\'test_size\\': test_size,\\n                    \\'accuracy\\': eval_results[\\'eval_accuracy\\']\\n                })\\n\\n    best_result = max(results, key=lambda x: x[\\'accuracy\\'])\\n    return best_result, results\\n\\n# Define parameter grid\\nparam_grid = {\\n    \\'batch_sizes\\': [8, 16, 32],\\n    \\'learning_rates\\': [2e-5, 3e-5, 5e-5],\\n    \\'test_sizes\\': [0.1, 0.2, 0.3]\\n}\\n\\n# Execute the grid search\\nbest_parameters, all_results = grid_search(tokenizer, model, training_data_pd, param_grid)\\nprint(\"Best Parameters:\", best_parameters)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CamembertTokenizer, CamembertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset, load_metric\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\"\"\"\n",
        "# Load the tokenizer and the model\n",
        "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
        "model = CamembertForSequenceClassification.from_pretrained('camembert-base', num_labels=6)\n",
        "\n",
        "# Encode data function\n",
        "def encode_data(tokenizer, df):\n",
        "    texts = df['sentence'].tolist()\n",
        "    labels = df['difficulty'].map({'A1': 0, 'A2': 1, 'B1': 2, 'B2': 3, 'C1': 4, 'C2': 5}).tolist()\n",
        "    encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=128)\n",
        "\n",
        "    df_encoded = pd.DataFrame({\n",
        "        'input_ids': list(encodings['input_ids']),\n",
        "        'attention_mask': list(encodings['attention_mask']),\n",
        "        'labels': labels\n",
        "    })\n",
        "\n",
        "    return Dataset.from_pandas(df_encoded)\n",
        "\n",
        "# Compute metrics function\n",
        "def compute_metrics(eval_pred):\n",
        "    metric = load_metric(\"accuracy\", trust_remote_code=True)\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "    return {'accuracy': metric.compute(predictions=predictions, references=labels)['accuracy']}\n",
        "\n",
        "# Train and validation split\n",
        "train_data, val_data = train_test_split(training_data_pd, test_size=0.1, random_state=42)\n",
        "train_dataset = encode_data(tokenizer, train_data)\n",
        "val_dataset = encode_data(tokenizer, val_data)\n",
        "\n",
        "# Training arguments using the best parameters\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=6,\n",
        "    per_device_train_batch_size=16,\n",
        "    learning_rate=3e-5,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.02,\n",
        "    logging_dir='./logs',\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='accuracy',\n",
        "    greater_is_better=True,\n",
        "    fp16=True\n",
        ")\n",
        "\n",
        "# Initialize Trainer with EarlyStoppingCallback\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluation on the validation set\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"Validation Results:\", eval_results)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "d6oMMSuES9cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "1a2faec0-9014-4f0c-f6c1-abf0322f95e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Load the tokenizer and the model\\ntokenizer = CamembertTokenizer.from_pretrained(\\'camembert-base\\')\\nmodel = CamembertForSequenceClassification.from_pretrained(\\'camembert-base\\', num_labels=6)\\n\\n# Encode data function\\ndef encode_data(tokenizer, df):\\n    texts = df[\\'sentence\\'].tolist()\\n    labels = df[\\'difficulty\\'].map({\\'A1\\': 0, \\'A2\\': 1, \\'B1\\': 2, \\'B2\\': 3, \\'C1\\': 4, \\'C2\\': 5}).tolist()\\n    encodings = tokenizer(texts, truncation=True, padding=\\'max_length\\', max_length=128)\\n\\n    df_encoded = pd.DataFrame({\\n        \\'input_ids\\': list(encodings[\\'input_ids\\']),\\n        \\'attention_mask\\': list(encodings[\\'attention_mask\\']),\\n        \\'labels\\': labels\\n    })\\n\\n    return Dataset.from_pandas(df_encoded)\\n\\n# Compute metrics function\\ndef compute_metrics(eval_pred):\\n    metric = load_metric(\"accuracy\", trust_remote_code=True)\\n    logits, labels = eval_pred\\n    predictions = np.argmax(logits, axis=1)\\n    return {\\'accuracy\\': metric.compute(predictions=predictions, references=labels)[\\'accuracy\\']}\\n\\n# Train and validation split\\ntrain_data, val_data = train_test_split(training_data_pd, test_size=0.1, random_state=42)\\ntrain_dataset = encode_data(tokenizer, train_data)\\nval_dataset = encode_data(tokenizer, val_data)\\n\\n# Training arguments using the best parameters\\ntraining_args = TrainingArguments(\\n    output_dir=\\'./results\\',\\n    num_train_epochs=6,\\n    per_device_train_batch_size=16,\\n    learning_rate=3e-5,\\n    warmup_steps=500,\\n    weight_decay=0.02,\\n    logging_dir=\\'./logs\\',\\n    evaluation_strategy=\\'epoch\\',\\n    save_strategy=\\'epoch\\',\\n    load_best_model_at_end=True,\\n    metric_for_best_model=\\'accuracy\\',\\n    greater_is_better=True,\\n    fp16=True\\n)\\n\\n# Initialize Trainer with EarlyStoppingCallback\\ntrainer = Trainer(\\n    model=model,\\n    args=training_args,\\n    train_dataset=train_dataset,\\n    eval_dataset=val_dataset,\\n    compute_metrics=compute_metrics,\\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\\n)\\n\\n# Train the model\\ntrainer.train()\\n\\n# Evaluation on the validation set\\neval_results = trainer.evaluate()\\nprint(\"Validation Results:\", eval_results)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CamemBERT 57.5%\n",
        "\"\"\"\n",
        "from transformers import CamembertTokenizer, CamembertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset, load_metric\n",
        "import numpy as np\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
        "\n",
        "# Function to encode data\n",
        "def encode_data(tokenizer, df):\n",
        "    texts = df['sentence'].tolist()\n",
        "    labels = df['difficulty'].map({'A1': 0, 'A2': 1, 'B1': 2, 'B2': 3, 'C1': 4, 'C2': 5}).tolist()\n",
        "    encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=128)\n",
        "    return Dataset.from_dict({\n",
        "        'input_ids': encodings['input_ids'],\n",
        "        'attention_mask': encodings['attention_mask'],\n",
        "        'labels': labels\n",
        "    })\n",
        "\n",
        "# Split the DataFrame into training and validation sets\n",
        "train_data, val_data = train_test_split(training_data_pd, test_size=0.10, random_state=42)\n",
        "\n",
        "# Tokenize and prepare datasets\n",
        "train_dataset = encode_data(tokenizer, train_data)\n",
        "val_dataset = encode_data(tokenizer, val_data)\n",
        "\n",
        "# Load the model\n",
        "model = CamembertForSequenceClassification.from_pretrained('camembert-base', num_labels=6)\n",
        "\n",
        "# Metric for evaluation\n",
        "def compute_metrics(eval_pred):\n",
        "    metric = load_metric(\"accuracy\")\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    accuracy = metric.compute(predictions=predictions, references=labels)\n",
        "    return {\"accuracy\": accuracy['accuracy']}\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=9,  # Slight adjustment if needed based on results\n",
        "    per_device_train_batch_size=12,  # A bit smaller to adjust gradient updates\n",
        "    warmup_steps=500,  # Adjust based on the total number of steps (num_epochs * total_data / batch_size)\n",
        "    weight_decay=0.02,  # Good for regularization\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,  # Less frequent to reduce logging overhead\n",
        "    learning_rate=3e-5,  # Adjusted for potentially better convergence\n",
        "    fp16=True,  # Make sure your hardware supports FP16 for this to be effective\n",
        "    evaluation_strategy=\"epoch\",  # Change to steps if you need more frequent evaluation\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='accuracy',\n",
        "    greater_is_better=True\n",
        ")\n",
        "\n",
        "# Initialize the Trainer with added compute_metrics for dynamic metric calculation\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,  # Ensure metrics are computed during evaluation\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        "\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\"\"\"\n",
        "#59%\n",
        "\"\"\"\n",
        "from transformers import CamembertTokenizer, CamembertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset, load_metric\n",
        "import numpy as np\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
        "\n",
        "# Function to encode data\n",
        "def encode_data(tokenizer, df):\n",
        "    texts = df['sentence'].tolist()\n",
        "    labels = df['difficulty'].map({'A1': 0, 'A2': 1, 'B1': 2, 'B2': 3, 'C1': 4, 'C2': 5}).tolist()\n",
        "    encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=128)\n",
        "    return Dataset.from_dict({\n",
        "        'input_ids': encodings['input_ids'],\n",
        "        'attention_mask': encodings['attention_mask'],\n",
        "        'labels': labels\n",
        "    })\n",
        "\n",
        "# Split the DataFrame into training and validation sets\n",
        "train_data, val_data = train_test_split(training_data_pd, test_size=0.10, random_state=42)\n",
        "\n",
        "# Tokenize and prepare datasets\n",
        "train_dataset = encode_data(tokenizer, train_data)\n",
        "val_dataset = encode_data(tokenizer, val_data)\n",
        "\n",
        "# Load the model\n",
        "model = CamembertForSequenceClassification.from_pretrained('camembert-base', num_labels=6)\n",
        "\n",
        "# Metric for evaluation\n",
        "def compute_metrics(eval_pred):\n",
        "    metric = load_metric(\"accuracy\")\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    accuracy = metric.compute(predictions=predictions, references=labels)\n",
        "    return {\"accuracy\": accuracy['accuracy']}\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=6,  # Slight adjustment if needed based on results\n",
        "    per_device_train_batch_size=22,  # A bit smaller to adjust gradient updates\n",
        "    warmup_steps=500,  # Adjust based on the total number of steps (num_epochs * total_data / batch_size)\n",
        "    weight_decay=0.01,  # Good for regularization\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,  # Less frequent to reduce logging overhead\n",
        "    learning_rate=15e-5,  # Adjusted for potentially better convergence\n",
        "    fp16=True,  # Make sure your hardware supports FP16 for this to be effective\n",
        "    evaluation_strategy=\"epoch\",  # Change to steps if you need more frequent evaluation\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='accuracy',\n",
        "    greater_is_better=True\n",
        ")\n",
        "\n",
        "# Initialize the Trainer with added compute_metrics for dynamic metric calculation\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,  # Ensure metrics are computed during evaluation\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "from transformers import CamembertTokenizer, CamembertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset, load_metric\n",
        "import numpy as np\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
        "\n",
        "# Function to encode data\n",
        "def encode_data(tokenizer, df):\n",
        "    texts = df['sentence'].tolist()\n",
        "    labels = df['difficulty'].map({'A1': 0, 'A2': 1, 'B1': 2, 'B2': 3, 'C1': 4, 'C2': 5}).tolist()\n",
        "    encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=128)\n",
        "    return Dataset.from_dict({\n",
        "        'input_ids': encodings['input_ids'],\n",
        "        'attention_mask': encodings['attention_mask'],\n",
        "        'labels': labels\n",
        "    })\n",
        "\n",
        "# Split the DataFrame into training and validation sets\n",
        "train_data, val_data = train_test_split(training_data_pd, test_size=0.10, random_state=42)\n",
        "\n",
        "# Tokenize and prepare datasets\n",
        "train_dataset = encode_data(tokenizer, train_data)\n",
        "val_dataset = encode_data(tokenizer, val_data)\n",
        "\n",
        "# Load the model\n",
        "model = CamembertForSequenceClassification.from_pretrained('camembert-base', num_labels=6)\n",
        "\n",
        "# Metric for evaluation\n",
        "def compute_metrics(eval_pred):\n",
        "    metric = load_metric(\"accuracy\")\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    accuracy = metric.compute(predictions=predictions, references=labels)\n",
        "    return {\"accuracy\": accuracy['accuracy']}\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=8,  # Slight adjustment if needed based on results\n",
        "    per_device_train_batch_size=32,  # A bit smaller to adjust gradient updates\n",
        "    warmup_steps=500,  # Adjust based on the total number of steps (num_epochs * total_data / batch_size)\n",
        "    weight_decay=0.01,  # Good for regularization\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,  # Less frequent to reduce logging overhead\n",
        "    learning_rate=18e-5,  # Adjusted for potentially better convergence\n",
        "    fp16=True,  # Make sure your hardware supports FP16 for this to be effective\n",
        "    evaluation_strategy=\"epoch\",  # Change to steps if you need more frequent evaluation\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='accuracy',\n",
        "    greater_is_better=True\n",
        ")\n",
        "\n",
        "# Initialize the Trainer with added compute_metrics for dynamic metric calculation\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,  # Ensure metrics are computed during evaluation\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        "\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "id": "jQPQegO5f5QG",
        "outputId": "1aca69ec-8f5f-41d3-cc7c-daa6fc447a2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='269' max='1080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 269/1080 01:06 < 03:21, 4.02 it/s, Epoch 1.99/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.612400</td>\n",
              "      <td>1.278122</td>\n",
              "      <td>0.485417</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-16eb37d96fe5>\u001b[0m in \u001b[0;36m<cell line: 212>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1857\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1858\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1859\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1860\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2202\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2203\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3145\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3146\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3147\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3149\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2009\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2011\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2012\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2013\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thMr-dc8aX__",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "c1a1dcfe-8c3a-4566-9523-8ffc34bc743f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "from transformers import CamembertTokenizer\n",
        "import numpy as np\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
        "\n",
        "# Tokenize the sentences from the 'sentence' column, specifying max_length\n",
        "max_length = 512  # Typical max length for BERT-based models\n",
        "unlabelled_encodings = tokenizer(list(unlabelled_data_pd['sentence']), truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
        "\n",
        "# Assuming 'trainer' is your trained model's Trainer instance\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: val[idx] for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "# Create a dataset from the tokenized data\n",
        "dataset = SimpleDataset(unlabelled_encodings)\n",
        "\n",
        "# Predict using the trainer\n",
        "predictions = trainer.predict(dataset)\n",
        "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
        "\n",
        "# Define a mapping from numeric labels to categories\n",
        "difficulty_levels = {0: 'A1', 1: 'A2', 2: 'B1', 3: 'B2', 4: 'C1', 5: 'C2'}\n",
        "predicted_difficulties = [difficulty_levels[label] for label in predicted_labels]\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame for submission\n",
        "submission_df = pd.DataFrame({\n",
        "    'id': unlabelled_data_pd['id'],\n",
        "    'difficulty': predicted_difficulties\n",
        "})\n",
        "\n",
        "# Save the DataFrame to a CSV file for submission\n",
        "submission_df.to_csv('/content/drive/My Drive/Colab Notebooks/2024_Data_science/Final_Project_Kaggle_Competition/submission_CamemBERT_12.csv', index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WCjfnOQM8Fk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "210a6df9-ec36-4e77-aebc-dc06116c77d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 52K\n",
            "drwxr-xr-x  2 root root 4.0K May  8 12:48 camembert-finetuned-batch_size_16\n",
            "drwxr-xr-x  2 root root 4.0K May  8 12:51 camembert-finetuned-batch_size_18\n",
            "drwxr-xr-x  2 root root 4.0K May  8 12:55 camembert-finetuned-batch_size_20\n",
            "drwxr-xr-x  2 root root 4.0K May  8 13:00 camembert-finetuned-batch_size_22\n",
            "drwxr-xr-x  2 root root 4.0K May  8 13:03 camembert-finetuned-batch_size_24\n",
            "drwxr-xr-x  2 root root 4.0K May  8 13:43 camembert-finetuned-batch_size_26\n",
            "drwxr-xr-x  2 root root 4.0K May  8 13:46 camembert-finetuned-batch_size_32\n",
            "drwxr-xr-x  2 root root 4.0K May  8 13:50 camembert-finetuned-batch_size_38\n",
            "drwxr-xr-x  2 root root 4.0K May  8 13:53 camembert-finetuned-batch_size_48\n",
            "drwx------  6 root root 4.0K May  8 12:25 drive\n",
            "drwxr-xr-x  2 root root 4.0K May  8 15:32 logs\n",
            "drwxr-xr-x 58 root root 4.0K May  8 15:37 results\n",
            "drwxr-xr-x  1 root root 4.0K May  6 13:19 sample_data\n",
            "total 40K\n",
            "drwxr-xr-x 2 root root 4.0K May  8 12:48 camembert-finetuned-batch_size_16\n",
            "drwxr-xr-x 2 root root 4.0K May  8 12:51 camembert-finetuned-batch_size_18\n",
            "drwxr-xr-x 2 root root 4.0K May  8 12:55 camembert-finetuned-batch_size_20\n",
            "drwxr-xr-x 2 root root 4.0K May  8 13:00 camembert-finetuned-batch_size_22\n",
            "drwxr-xr-x 2 root root 4.0K May  8 13:03 camembert-finetuned-batch_size_24\n",
            "drwxr-xr-x 2 root root 4.0K May  8 13:43 camembert-finetuned-batch_size_26\n",
            "drwxr-xr-x 2 root root 4.0K May  8 13:46 camembert-finetuned-batch_size_32\n",
            "drwxr-xr-x 2 root root 4.0K May  8 13:50 camembert-finetuned-batch_size_38\n",
            "drwxr-xr-x 2 root root 4.0K May  8 13:53 camembert-finetuned-batch_size_48\n",
            "drwx------ 6 root root 4.0K May  8 12:25 drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!ls -lh\n",
        "\n",
        "!rm -rf /content/logs\n",
        "\n",
        "!rm -rf /content/results\n",
        "\n",
        "!rm -rf /content/sample_data\n",
        "\n",
        "!ls -lh\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}